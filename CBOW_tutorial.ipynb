{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-22T06:24:16.780497600Z",
     "start_time": "2024-01-22T06:24:06.601168500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x12784843290>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T21:36:30.256360900Z",
     "start_time": "2024-01-21T21:36:30.158120600Z"
    }
   },
   "id": "c80aab416f00febc",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "115"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T21:37:05.975568900Z",
     "start_time": "2024-01-21T21:37:05.941246500Z"
    }
   },
   "id": "b343b33dc06c7ec7",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[(['forty', 'When'], 'winters'),\n (['winters', 'forty'], 'shall'),\n (['shall', 'winters'], 'besiege')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams = [\n",
    "    (\n",
    "        [test_sentence[i - j -1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "\n",
    "n_grams[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T21:38:47.163979700Z",
     "start_time": "2024-01-21T21:38:47.121442800Z"
    }
   },
   "id": "4d6be42cff9f46b5",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocabulary = set(test_sentence)\n",
    "vocab_dict = {word: i for i, word in enumerate(vocabulary)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T21:41:21.228298400Z",
     "start_time": "2024-01-21T21:41:21.172500200Z"
    }
   },
   "id": "49ff2f8248560b4e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Where': 0,\n 'the': 1,\n \"totter'd\": 2,\n 'use,': 3,\n 'own': 4,\n 'and': 5,\n 'Proving': 6,\n 'How': 7,\n 'on': 8,\n \"beauty's\": 9,\n 'worth': 10,\n 'much': 11,\n 'make': 12,\n 'art': 13,\n 'child': 14,\n 'Will': 15,\n 'an': 16,\n 'fair': 17,\n 'blood': 18,\n \"excuse,'\": 19,\n 'eyes,': 20,\n 'praise.': 21,\n 'made': 22,\n 'lies,': 23,\n 'couldst': 24,\n 'be': 25,\n \"feel'st\": 26,\n 'by': 27,\n 'deep': 28,\n 'being': 29,\n 'dig': 30,\n 'all': 31,\n 'cold.': 32,\n 'besiege': 33,\n 'thine!': 34,\n \"'This\": 35,\n 'warm': 36,\n 'And': 37,\n 'thine': 38,\n 'asked,': 39,\n 'days;': 40,\n 'To': 41,\n 'so': 42,\n 'shame,': 43,\n \"youth's\": 44,\n 'thou': 45,\n 'mine': 46,\n 'gazed': 47,\n 'it': 48,\n \"deserv'd\": 49,\n 'a': 50,\n 'see': 51,\n 'winters': 52,\n 'in': 53,\n 'where': 54,\n 'beauty': 55,\n 'proud': 56,\n 'sunken': 57,\n 'say,': 58,\n 'shall': 59,\n 'to': 60,\n 'old': 61,\n 'Thy': 62,\n 'small': 63,\n 'all-eating': 64,\n 'brow,': 65,\n 'old,': 66,\n 'lusty': 67,\n 'within': 68,\n 'field,': 69,\n 'my': 70,\n 'livery': 71,\n 'treasure': 72,\n 'his': 73,\n 'were': 74,\n 'Shall': 75,\n 'When': 76,\n 'Were': 77,\n 'answer': 78,\n 'count,': 79,\n 'when': 80,\n 'succession': 81,\n 'of': 82,\n 'more': 83,\n 'now,': 84,\n 'Then': 85,\n 'weed': 86,\n 'sum': 87,\n 'trenches': 88,\n 'praise': 89,\n 'held:': 90,\n 'forty': 91,\n 'thriftless': 92,\n 'This': 93,\n 'If': 94,\n 'new': 95,\n 'thy': 96}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T21:41:29.595549100Z",
     "start_time": "2024-01-21T21:41:29.548834100Z"
    }
   },
   "id": "567a7990bb449c57",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGramModel(\n",
      "  (embedding): Embedding(97, 10)\n",
      "  (linear1): Linear(in_features=20, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=97, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NGramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "model = NGramModel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T22:08:48.972311600Z",
     "start_time": "2024-01-21T22:08:48.918730200Z"
    }
   },
   "id": "f8b2dee2b348f444",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T22:00:55.862380300Z",
     "start_time": "2024-01-21T22:00:54.419461200Z"
    }
   },
   "id": "b2181832e9ceb876",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[521.0756077766418, 521.0756077766418, 521.0756077766418, 521.0756077766418, 521.0756077766418, 521.0756077766418, 521.0756077766418, 521.0756077766418, 521.0756077766418, 521.0756077766418]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in n_grams:\n",
    "        \n",
    "        context_indexes = torch.tensor([vocab_dict[w] for w in context], dtype=torch.long)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_probs = model(context_indexes)\n",
    "        \n",
    "        loss = loss_function(log_probs, torch.tensor([vocab_dict[target]], dtype=torch.long))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "print(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T22:09:32.590856700Z",
     "start_time": "2024-01-21T22:09:30.956397600Z"
    }
   },
   "id": "54cc9dd5d6bbb3c0",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1496, -1.1600, -0.6482,  1.6195, -0.5876,  0.2096,  0.6177, -1.1770,\n",
      "        -1.3879, -0.5300], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embedding.weight[vocab_dict[\"beauty\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T22:11:35.347391200Z",
     "start_time": "2024-01-21T22:11:35.309531Z"
    }
   },
   "id": "3b2ccd829e3705e",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T06:24:57.448554600Z",
     "start_time": "2024-01-22T06:24:57.246673200Z"
    }
   },
   "id": "234c89be2cdbc452",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[(['When', 'forty', 'shall', 'besiege'], 'winters'),\n (['forty', 'winters', 'besiege', 'thy'], 'shall'),\n (['winters', 'shall', 'thy', 'brow,'], 'besiege')]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams = []\n",
    "\n",
    "for i in range(CONTEXT_SIZE, len(test_sentence)-CONTEXT_SIZE):\n",
    "    context_array = test_sentence[i-CONTEXT_SIZE:i] + test_sentence[i+1:i+1+CONTEXT_SIZE]\n",
    "    target = test_sentence[i]\n",
    "    ngrams.append((context_array, target))\n",
    "    \n",
    "ngrams[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T06:35:35.711674300Z",
     "start_time": "2024-01-22T06:35:35.620177200Z"
    }
   },
   "id": "aa25790aa097f5a7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "vocab_dict = {word: i for i, word in enumerate(vocab)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T06:37:44.047714Z",
     "start_time": "2024-01-22T06:37:43.841380800Z"
    }
   },
   "id": "91f4a92931405cbb",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'say,': 0,\n 'praise': 1,\n 'lusty': 2,\n 'a': 3,\n 'all-eating': 4,\n 'How': 5,\n 'blood': 6,\n 'of': 7,\n 'If': 8,\n 'old,': 9,\n 'being': 10,\n 'on': 11,\n \"totter'd\": 12,\n 'winters': 13,\n 'small': 14,\n 'by': 15,\n 'mine': 16,\n 'make': 17,\n 'child': 18,\n 'in': 19,\n 'thine': 20,\n 'an': 21,\n 'couldst': 22,\n 'it': 23,\n 'when': 24,\n 'Thy': 25,\n 'treasure': 26,\n 'livery': 27,\n 'within': 28,\n \"excuse,'\": 29,\n 'sunken': 30,\n 'use,': 31,\n 'count,': 32,\n 'made': 33,\n 'sum': 34,\n 'cold.': 35,\n 'forty': 36,\n 'proud': 37,\n 'Will': 38,\n 'much': 39,\n 'were': 40,\n \"youth's\": 41,\n 'asked,': 42,\n 'shall': 43,\n 'succession': 44,\n 'and': 45,\n 'the': 46,\n 'eyes,': 47,\n \"deserv'd\": 48,\n 'When': 49,\n 'see': 50,\n 'old': 51,\n 'where': 52,\n 'Proving': 53,\n 'lies,': 54,\n 'fair': 55,\n 'trenches': 56,\n 'answer': 57,\n 'be': 58,\n 'my': 59,\n 'to': 60,\n 'beauty': 61,\n 'This': 62,\n 'To': 63,\n 'thy': 64,\n 'thriftless': 65,\n 'held:': 66,\n 'all': 67,\n 'thou': 68,\n 'warm': 69,\n 'brow,': 70,\n \"'This\": 71,\n 'weed': 72,\n 'shame,': 73,\n 'praise.': 74,\n 'besiege': 75,\n \"feel'st\": 76,\n 'now,': 77,\n 'so': 78,\n 'gazed': 79,\n 'dig': 80,\n 'Were': 81,\n 'field,': 82,\n 'his': 83,\n 'Where': 84,\n \"beauty's\": 85,\n 'deep': 86,\n 'And': 87,\n 'more': 88,\n 'Then': 89,\n 'days;': 90,\n 'Shall': 91,\n 'own': 92,\n 'new': 93,\n 'art': 94,\n 'thine!': 95,\n 'worth': 96}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T06:37:49.724291300Z",
     "start_time": "2024-01-22T06:37:49.614021800Z"
    }
   },
   "id": "99f77e4afd16a630",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embedding): Embedding(97, 16)\n",
      "  (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=97, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model defining\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(4 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x).view((1,-1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "model = CBOW(len(vocab), 16, CONTEXT_SIZE)\n",
    "print(model)     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T07:02:31.391136Z",
     "start_time": "2024-01-22T07:02:31.220859Z"
    }
   },
   "id": "59da1564cd5fe298",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss().to(\"cuda\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.004)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T07:05:55.177432400Z",
     "start_time": "2024-01-22T07:05:55.067808Z"
    }
   },
   "id": "47bab47a57861a24",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.11149177812711075\n",
      "epoch: 1, loss: 0.11026416883409561\n",
      "epoch: 2, loss: 0.10906095314468886\n",
      "epoch: 3, loss: 0.10788093629788172\n",
      "epoch: 4, loss: 0.10672344868113329\n",
      "epoch: 5, loss: 0.10558738656811886\n",
      "epoch: 6, loss: 0.10447313071035587\n",
      "epoch: 7, loss: 0.10337932980074002\n",
      "epoch: 8, loss: 0.10230545209603267\n",
      "epoch: 9, loss: 0.1012517719923913\n",
      "epoch: 10, loss: 0.10021716805997195\n",
      "epoch: 11, loss: 0.09920044045324798\n",
      "epoch: 12, loss: 0.09820307688871482\n",
      "epoch: 13, loss: 0.09722275597353776\n",
      "epoch: 14, loss: 0.09625994957782127\n",
      "epoch: 15, loss: 0.09531420835100853\n",
      "epoch: 16, loss: 0.09438442019326193\n",
      "epoch: 17, loss: 0.093471972070433\n",
      "epoch: 18, loss: 0.09257390776330286\n",
      "epoch: 19, loss: 0.09169150190847414\n",
      "epoch: 20, loss: 0.09082375319154413\n",
      "epoch: 21, loss: 0.08997274960416395\n",
      "epoch: 22, loss: 0.0891333321394684\n",
      "epoch: 23, loss: 0.08830914575908635\n",
      "epoch: 24, loss: 0.0874976299468193\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "        \n",
    "        context_indexes = torch.tensor([vocab_dict[w] for w in context], dtype=torch.long)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_probabilities = model(context_indexes)\n",
    "        \n",
    "        loss = loss_function(log_probabilities, torch.tensor([vocab_dict[target]], dtype=torch.long))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"epoch: {epoch}, loss: {total_loss/len(ngrams)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T07:06:43.673460Z",
     "start_time": "2024-01-22T07:06:39.009669500Z"
    }
   },
   "id": "7d92d1f6e02a2d5a",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.5675,  1.4600,  0.8537,  1.5127,  1.0416, -0.5369, -0.0963,  0.8950,\n        -0.3495, -3.6296, -0.3141, -1.0946,  0.1261,  2.0901,  0.5000,  0.1557],\n       grad_fn=<SelectBackward0>)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight[vocab_dict[\"beauty\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T07:08:42.002032900Z",
     "start_time": "2024-01-22T07:08:41.929331200Z"
    }
   },
   "id": "26a9562aa5bd69cb",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['When', 'forty', 'shall', 'besiege']"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T07:10:32.366658700Z",
     "start_time": "2024-01-22T07:10:32.300379600Z"
    }
   },
   "id": "50887433a8699c58",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92m actual answer: winters\n",
      "\u001B[94m predicted answer: winters\n"
     ]
    }
   ],
   "source": [
    "log_prob = model(torch.tensor([vocab_dict[w] for w in ngrams[0][0]], dtype=torch.long))\n",
    "print(f\"\\033[92m actual answer: {ngrams[0][1]}\")\n",
    "print(f\"\\033[94m predicted answer: {list(vocab)[log_prob.argmax().item()]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T07:21:33.761685800Z",
     "start_time": "2024-01-22T07:21:33.622892700Z"
    }
   },
   "id": "4a892a0b298f11a1",
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
